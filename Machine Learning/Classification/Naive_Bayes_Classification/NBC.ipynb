{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd0035d",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier\n",
    "\n",
    "Naive Bayes is a **supervised machine learning algorithm** that calculates the probability of a certain outcome based on the features of your data using Bayes' theorem from probability theory. It's called \"naive\" because it assumes all features are independent of each other, which simplifies the math even though this assumption is rarely true in real life. The algorithm calculates the probability of each class given the feature values, and then selects the class with the highest probability. It first learns the probability distributions of each feature for each class from the training data. Despite its simplistic assumption, Naive Bayes often works surprisingly well, especially with high-dimensional data like text classification.\n",
    "\n",
    "*   **Use Cases:** Use Naive Bayes when you need a fast algorithm that works well with high-dimensional data, particularly for text classification problems like spam detection or sentiment analysis. It's effective when you have limited training data and need a simple baseline model. Naive Bayes assumes that features are conditionally independent given the class label (the \"naive\" assumption). It assumes that the presence or absence of specific features is what matters, not their interactions. For Gaussian Naive Bayes, it assumes features follow a normal distribution within each class. Naive Bayes works best when the independence assumption is not severely violated or when the benefits of the algorithm's simplicity outweigh the cost of this assumption.\n",
    "\n",
    "*   **Pros:**\n",
    "    - Extremely fast training and prediction, even with large datasets\n",
    "    - Works well with high-dimensional data (like text classification)\n",
    "    - Requires relatively small amount of training data to estimate parameters\n",
    "    - Not sensitive to irrelevant features (they tend to \"cancel out\")\n",
    "    - Handles multi-class problems naturally\n",
    "    - Performs surprisingly well despite its simplistic assumptions\n",
    "\n",
    "*   **Cons:**\n",
    "    - The \"naive\" independence assumption is rarely true in real-world data\n",
    "    - Doesn't capture feature interactions\n",
    "    - Cannot learn complex relationships between features\n",
    "    - May be outperformed by more sophisticated models when sufficient data is available\n",
    "    - May produce poor probability estimates (though classifications may still be correct)\n",
    "    - Zero-frequency problem: if a categorical variable has a category in test data that was not in training data, model will assign zero probability\n",
    "\n",
    "    | **Best Practice**                                                                        |\n",
    "    | ---------------------------------------------------------------------------------------- |\n",
    "    | Apply Laplace smoothing (`alpha=1.0`) to avoid zero probabilities.                       |\n",
    "    | Choose correct variant: Gaussian (continuous), Multinomial (counts), Bernoulli (binary). |\n",
    "    | Use log probabilities to avoid numerical underflow (most libraries do this).             |\n",
    "    | Use `class_prior` or prior knowledge to guide the model.                                 |\n",
    "    | Validate with metrics like F1-score, precision, and recall.                              |\n",
    "    | Apply `var_smoothing` in GaussianNB for numerical stability (e.g., 1e-9).                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6886c218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet pandas numpy matplotlib seaborn scikit-learn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6071736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
